#!/usr/bin/env python3
"""
è™•ç†å–®ä¸€è³‡æ–™å¤¾ä¸¦ä½¿ç”¨LangChain Parent-Child RAGç³»çµ±
é¿å…é‡è¤‡è™•ç†åœ–ç‰‡
"""

import sys
import os
import logging
import time
import hashlib
from pathlib import Path
from typing import Set, List

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.langchain_rag_system import LangChainParentChildRAG
from config.config import Config

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_file_hash(file_path: Path) -> str:
    """è¨ˆç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def is_file_already_processed(file_path: Path) -> bool:
    """æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç¶“è™•ç†éŽï¼ˆåŸºæ–¼markdownæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰"""
    file_stem = file_path.stem
    output_dir = Path("outputs/images/zerox_output")

    print(f"  ðŸ” æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²è™•ç†: {file_stem}")

    # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„markdownæ–‡ä»¶
    # è€ƒæ…®å„ç¨®å¯èƒ½çš„å‘½åæ¨¡å¼
    possible_patterns = [
        f"{file_stem}.md",
        f"{file_stem}_converted.md",
        f"{file_stem}__converted.md",  # é›™åº•ç·šæƒ…æ³
        f"{file_stem.rstrip('.')}_converted.md",  # ç§»é™¤æœ«å°¾çš„é»ž
        f"{file_stem.rstrip('.')}__converted.md"   # ç§»é™¤æœ«å°¾çš„é»ž + é›™åº•ç·š
    ]

    for pattern in possible_patterns:
        md_path = output_dir / pattern
        if md_path.exists():
            print(f"  âœ… æ‰¾åˆ°å·²è™•ç†çš„markdownæ–‡ä»¶: {pattern}")
            return True

    # å¦‚æžœæ²’æœ‰æ‰¾åˆ°ç²¾ç¢ºåŒ¹é…ï¼Œå˜—è©¦æ¨¡ç³ŠåŒ¹é…
    # æª¢æŸ¥æ˜¯å¦æœ‰åŒ…å«æ–‡ä»¶åçš„markdownæ–‡ä»¶
    base_name = file_stem.rstrip('.').replace('..', '_').replace('.', '_')

    print(f"  ðŸ” å˜—è©¦æ¨¡ç³ŠåŒ¹é…ï¼ŒåŸºç¤Žåç¨±: {base_name}")

    for md_file in output_dir.glob("*.md"):
        md_stem = md_file.stem
        # æª¢æŸ¥å¤šç¨®åŒ¹é…æ¨¡å¼
        if (base_name in md_stem or
            md_stem.replace('_', '').replace('.', '') == base_name.replace('_', '').replace('.', '') or
            any(part in md_stem for part in base_name.split('_') if len(part) > 2)):
            print(f"  âœ… æ‰¾åˆ°ç›¸é—œçš„markdownæ–‡ä»¶: {md_file.name}")
            return True

    print(f"  âŒ æœªæ‰¾åˆ°å°æ‡‰çš„markdownæ–‡ä»¶")
    return False

def find_unique_files(folder_path: Path) -> List[Path]:
    """æ‰¾åˆ°è³‡æ–™å¤¾ä¸­çš„å”¯ä¸€æ–‡ä»¶ï¼Œé¿å…é‡è¤‡ï¼ŒæŽ’é™¤è‡¨æ™‚è½‰æ›æ–‡ä»¶"""

    print(f"ðŸ” æŽƒæè³‡æ–™å¤¾: {folder_path}")

    if not folder_path.exists():
        print(f"âŒ è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder_path}")
        return []

    # æ”¯æŒçš„æ–‡ä»¶é¡žåž‹
    supported_extensions = {'.pdf', '.ppt', '.pptx', '.doc', '.docx'}

    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
    all_files = []
    for ext in supported_extensions:
        all_files.extend(folder_path.glob(f"*{ext}"))
        all_files.extend(folder_path.glob(f"**/*{ext}"))  # åŒ…å«å­ç›®éŒ„

    # éŽæ¿¾æŽ‰è‡¨æ™‚è½‰æ›æ–‡ä»¶
    filtered_files = []
    for file_path in all_files:
        # è·³éŽè‡¨æ™‚è½‰æ›çš„PDFæ–‡ä»¶
        if file_path.name.endswith('_converted.pdf') or file_path.name.endswith('__converted.pdf'):
            print(f"  â­ï¸  è·³éŽè‡¨æ™‚è½‰æ›æ–‡ä»¶: {file_path.name}")
            continue
        filtered_files.append(file_path)

    print(f"ðŸ“„ æ‰¾åˆ° {len(filtered_files)} å€‹æœ‰æ•ˆæ–‡ä»¶")

    # åŽ»é‡ï¼šä½¿ç”¨æ–‡ä»¶å“ˆå¸Œå€¼
    unique_files = []
    seen_hashes: Set[str] = set()

    for file_path in filtered_files:
        try:
            file_hash = get_file_hash(file_path)
            if file_hash not in seen_hashes:
                seen_hashes.add(file_hash)
                unique_files.append(file_path)
                print(f"  âœ… å”¯ä¸€æ–‡ä»¶: {file_path.name}")
            else:
                print(f"  âš ï¸  é‡è¤‡æ–‡ä»¶: {file_path.name} (è·³éŽ)")
        except Exception as e:
            print(f"  âŒ ç„¡æ³•è®€å–æ–‡ä»¶: {file_path.name} - {e}")

    print(f"ðŸ“Š åŽ»é‡å¾Œå‰©é¤˜ {len(unique_files)} å€‹å”¯ä¸€æ–‡ä»¶")
    return unique_files

def process_folder_with_langchain(folder_path: str):
    """è™•ç†æŒ‡å®šè³‡æ–™å¤¾ä¸¦ä½¿ç”¨LangChainç³»çµ±"""
    
    print("ðŸ”„ è™•ç†å–®ä¸€è³‡æ–™å¤¾ä¸¦ä½¿ç”¨LangChain Parent-Child RAGç³»çµ±...")
    print(f"ðŸ“ ç›®æ¨™è³‡æ–™å¤¾: {folder_path}")
    
    try:
        folder_path = Path(folder_path)
        
        # 1. æ‰¾åˆ°å”¯ä¸€æ–‡ä»¶
        unique_files = find_unique_files(folder_path)
        
        if not unique_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°å¯è™•ç†çš„æ–‡ä»¶")
            return
        
        # 2. åˆå§‹åŒ–LangChain RAGç³»çµ±
        collection_name = "JH-åœ–ç´™èªè­˜-langchain"
        print(f"ðŸ“Š åˆå§‹åŒ–LangChain RAGç³»çµ±: {collection_name}")
        langchain_rag = LangChainParentChildRAG(collection_name)
        
        # 3. æª¢æŸ¥æ˜¯å¦å·²æœ‰æ•¸æ“š
        if langchain_rag.has_vector_data():
            print("âš ï¸  LangChainé›†åˆå·²å­˜åœ¨æ•¸æ“š")
            print("é¸æ“‡è™•ç†æ¨¡å¼:")
            print("  1. ç–ŠåŠ æ¨¡å¼ - ä¿ç•™ç¾æœ‰æ•¸æ“šï¼Œåªè™•ç†æ–°æ–‡ä»¶ (æŽ¨è–¦)")
            print("  2. é‡æ–°è™•ç† - æ¸…é™¤ç¾æœ‰æ•¸æ“šï¼Œé‡æ–°è™•ç†æ‰€æœ‰æ–‡ä»¶")
            print("  3. å–æ¶ˆè™•ç†")

            while True:
                response = input("è«‹é¸æ“‡ (1/2/3): ").strip()
                if response == '1':
                    print("âœ… ä½¿ç”¨ç–ŠåŠ æ¨¡å¼ï¼Œä¿ç•™ç¾æœ‰æ•¸æ“š")
                    break
                elif response == '2':
                    print("âš ï¸  å°‡æ¸…é™¤ç¾æœ‰æ•¸æ“šä¸¦é‡æ–°è™•ç†")
                    # é€™è£¡å¯ä»¥æ·»åŠ æ¸…é™¤é‚è¼¯ï¼Œæš«æ™‚è·³éŽ
                    print("âŒ é‡æ–°è™•ç†æ¨¡å¼æš«æœªå¯¦ç¾ï¼Œè«‹ä½¿ç”¨ç–ŠåŠ æ¨¡å¼")
                    continue
                elif response == '3':
                    print("âŒ å–æ¶ˆè™•ç†")
                    return
                else:
                    print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹è¼¸å…¥ 1ã€2 æˆ– 3")
        
        # 4. è‡¨æ™‚ä¿®æ”¹é…ç½®ä½¿ç”¨LangChain
        original_rag_type = Config.RAG_SYSTEM_TYPE
        original_collection = Config.QDRANT_COLLECTION_NAME
        
        Config.RAG_SYSTEM_TYPE = "langchain"
        Config.QDRANT_COLLECTION_NAME = collection_name
        
        try:
            # 5. å°Žå…¥è™•ç†å™¨
            from src.processors.zerox_pdf_processor import ZeroxPDFProcessor
            from src.processors.file_converter import FileConverter

            # 6. åˆå§‹åŒ–è™•ç†å™¨
            processor = ZeroxPDFProcessor()
            converter = FileConverter()

            # 7. æ”¶é›†æ‰€æœ‰chunks
            all_chunks = []
            total_time = 0
            processed_files = 0

            for file_path in unique_files:
                print(f"\nðŸ”„ æª¢æŸ¥æ–‡ä»¶: {file_path.name}")

                # åœ¨ç–ŠåŠ æ¨¡å¼ä¸‹ï¼Œæª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç¶“è™•ç†éŽ
                if is_file_already_processed(file_path):
                    print(f"  â­ï¸  æ–‡ä»¶å·²è™•ç†éŽï¼Œè·³éŽ: {file_path.name}")
                    continue

                print(f"  ðŸ”„ é–‹å§‹è™•ç†æ–°æ–‡ä»¶: {file_path.name}")
                start_time = time.time()

                try:
                    # æª¢æŸ¥æ–‡ä»¶æ ¼å¼ï¼Œå¦‚æžœä¸æ˜¯PDFå‰‡å…ˆè½‰æ›
                    file_ext = file_path.suffix.lower()
                    pdf_path = str(file_path)

                    if file_ext != '.pdf':
                        print(f"  ðŸ“„ æª¢æ¸¬åˆ° {file_ext} æ ¼å¼ï¼Œæ­£åœ¨è½‰æ›ç‚ºPDF...")

                        # å‰µå»ºè‡¨æ™‚PDFæ–‡ä»¶è·¯å¾‘
                        temp_pdf_path = str(file_path.parent / f"{file_path.stem}_converted.pdf")

                        # ä½¿ç”¨FileConverterè½‰æ›ç‚ºPDF
                        converted_pdf = converter.convert_to_pdf(str(file_path), temp_pdf_path)

                        if not converted_pdf:
                            print(f"  âŒ æ ¼å¼è½‰æ›å¤±æ•—: {file_path.name}")
                            continue

                        pdf_path = converted_pdf
                        print(f"  âœ… æ ¼å¼è½‰æ›æˆåŠŸ: {Path(converted_pdf).name}")

                    # èª¿ç”¨Zeroxè™•ç†PDF
                    import asyncio
                    import warnings

                    # æŠ‘åˆ¶ asyncio è­¦å‘Š
                    warnings.filterwarnings("ignore", category=RuntimeWarning, message=".*Event loop is closed.*")

                    # ä½¿ç”¨æ–°çš„äº‹ä»¶å¾ªç’°ä¾†é¿å…è¡çª
                    try:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        chunks = loop.run_until_complete(processor.process_pdf(pdf_path))
                    finally:
                        # ç¢ºä¿æ­£ç¢ºé—œé–‰äº‹ä»¶å¾ªç’°
                        try:
                            # ç­‰å¾…æ‰€æœ‰å¾…è™•ç†çš„ä»»å‹™å®Œæˆ
                            pending = asyncio.all_tasks(loop)
                            if pending:
                                loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
                        except Exception:
                            pass
                        finally:
                            loop.close()

                    if chunks:
                        all_chunks.extend(chunks)

                        file_time = time.time() - start_time
                        total_time += file_time
                        processed_files += 1

                        print(f"  âœ… è™•ç†å®Œæˆ: {len(chunks)} å€‹æ®µè½, {file_time:.2f}ç§’")
                    else:
                        print(f"  âŒ è™•ç†å¤±æ•—: æ²’æœ‰ç”Ÿæˆæ®µè½")

                    # æ¸…ç†è‡¨æ™‚PDFæ–‡ä»¶
                    if file_ext != '.pdf' and Path(pdf_path).exists():
                        try:
                            Path(pdf_path).unlink()
                            print(f"  ðŸ—‘ï¸  æ¸…ç†è‡¨æ™‚æ–‡ä»¶: {Path(pdf_path).name}")
                        except Exception as cleanup_error:
                            print(f"  âš ï¸  æ¸…ç†è‡¨æ™‚æ–‡ä»¶å¤±æ•—: {cleanup_error}")

                except Exception as e:
                    print(f"  âŒ è™•ç†æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue

            # 8. å°‡æ‰€æœ‰chunksæ·»åŠ åˆ°LangChainç³»çµ±
            if all_chunks:
                print(f"\nðŸ”„ å°‡ {len(all_chunks)} å€‹æ®µè½æ·»åŠ åˆ°LangChainç³»çµ±...")
                langchain_start = time.time()

                langchain_result = langchain_rag.add_documents_from_zerox(all_chunks)

                langchain_time = time.time() - langchain_start
                total_time += langchain_time

                if langchain_result.get("success"):
                    print(f"  âœ… LangChainè™•ç†å®Œæˆ: {langchain_result.get('documents_added', 0)} å€‹æ–‡æª”")
                else:
                    print(f"  âŒ LangChainè™•ç†å¤±æ•—: {langchain_result.get('error')}")

            # 9. é¡¯ç¤ºç¸½çµ
            print(f"\nâœ… æ‰¹é‡è™•ç†å®Œæˆï¼")
            print(f"  è™•ç†æ–‡ä»¶æ•¸: {processed_files}/{len(unique_files)}")
            print(f"  åŽŸå§‹æ®µè½æ•¸: {len(all_chunks)}")
            print(f"  LangChainæ–‡æª”æ•¸: {langchain_result.get('documents_added', 0) if all_chunks else 0}")
            print(f"  ç¸½è™•ç†æ™‚é–“: {total_time:.2f}ç§’")
            print(f"  å¹³å‡æ¯æ–‡ä»¶: {total_time/processed_files:.2f}ç§’" if processed_files > 0 else "")
            print(f"  é›†åˆåç¨±: {collection_name}")

            # 10. æ¸¬è©¦LangChainæª¢ç´¢
            if all_chunks:
                print("\nðŸ” æ¸¬è©¦LangChainæª¢ç´¢æ•ˆæžœ...")
                test_queries = [
                    "åœ–ç´™",
                    "åœ–é¢è­˜åˆ¥",
                    "æŠ€è¡“åœ–ç´™",
                    "å·¥ç¨‹åœ–",
                    "è¨­è¨ˆåœ–"
                ]
                
                for query in test_queries:
                    print(f"\næŸ¥è©¢: {query}")
                    results = langchain_rag.retrieve_relevant_chunks(query, top_k=3)
                    print(f"  çµæžœæ•¸é‡: {len(results)}")
                    
                    for i, result in enumerate(results):
                        print(f"  {i+1}. ç›¸ä¼¼åº¦: {result.similarity_score:.3f}")
                        print(f"     ä¸»é¡Œ: {result.child_chunk.topic}")
                        print(f"     å…§å®¹: {result.child_chunk.content[:80]}...")
                
                # 11. æ›´æ–°é…ç½®å»ºè­°
                print(f"\nðŸ’¡ å¦‚æžœæ¸¬è©¦æ•ˆæžœè‰¯å¥½ï¼Œå¯ä»¥æ›´æ–°.envé…ç½®:")
                print(f"   QDRANT_COLLECTION_NAME={collection_name}")
                print(f"   RAG_SYSTEM_TYPE=langchain")
            
        finally:
            # æ¢å¾©åŽŸå§‹é…ç½®
            Config.RAG_SYSTEM_TYPE = original_rag_type
            Config.QDRANT_COLLECTION_NAME = original_collection
            
    except Exception as e:
        logger.error(f"è™•ç†éŽç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

def test_existing_langchain():
    """æ¸¬è©¦ç¾æœ‰çš„LangChainæ•¸æ“š"""
    
    print("ðŸ” æ¸¬è©¦ç¾æœ‰LangChainæ•¸æ“š...")
    
    try:
        collection_name = "JH-åœ–ç´™èªè­˜-langchain"
        langchain_rag = LangChainParentChildRAG(collection_name)
        
        if not langchain_rag.has_vector_data():
            print("âŒ æ²’æœ‰æ‰¾åˆ°LangChainæ•¸æ“š")
            return
        
        test_queries = [
            "åœ–ç´™",
            "åœ–é¢è­˜åˆ¥", 
            "æŠ€è¡“åœ–ç´™",
            "å·¥ç¨‹åœ–",
            "è¨­è¨ˆåœ–",
            "ç„ŠæŽ¥å·¥è—",
            "ææ–™"
        ]
        
        print(f"ðŸ“Š æ¸¬è©¦ {len(test_queries)} å€‹æŸ¥è©¢...")
        
        for query in test_queries:
            print(f"\næŸ¥è©¢: {query}")
            results = langchain_rag.retrieve_relevant_chunks(query, top_k=3)
            print(f"  çµæžœæ•¸é‡: {len(results)}")
            
            for i, result in enumerate(results):
                print(f"  {i+1}. ç›¸ä¼¼åº¦: {result.similarity_score:.3f}")
                print(f"     ä¸»é¡Œ: {result.child_chunk.topic}")
                print(f"     å…§å®¹: {result.child_chunk.content[:80]}...")
                
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    target_folder = "/home/chun/heph-dev/JH/å°ˆå®¶/å…¶ä»–"
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_existing_langchain()
    else:
        process_folder_with_langchain(target_folder)
