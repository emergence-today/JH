"""
RAGç³»çµ± - ä½¿ç”¨ LangChain æ¡†æ¶é‡æ§‹
åŸºæ–¼è™•ç†å¾Œçš„PPTå…§å®¹å»ºç«‹æ•™å­¸å‹chatbot
ä½¿ç”¨ Qdrant å‘é‡è³‡æ–™åº«å’Œ OpenAI API
"""

import json
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
import logging
import os

# LangChain imports
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.schema import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun

# Qdrant imports
from qdrant_client import QdrantClient, models
from qdrant_client.http.models import Filter, FieldCondition

# Local imports
from src.processors.pdf_processor import DocumentChunk
from config.config import Config
from src.core.meteor_utilities import MeteorUtilities

logger = logging.getLogger(__name__)

@dataclass
class RetrievalResult:
    """æª¢ç´¢çµæœ - ä¿æŒèˆ‡åŸç³»çµ±ç›¸å®¹"""
    chunk: DocumentChunk
    similarity_score: float
    relevance_reason: str

class QdrantRetriever(BaseRetriever):
    """è‡ªå®šç¾© Qdrant æª¢ç´¢å™¨ï¼Œä¿æŒèˆ‡åŸç³»çµ±çš„ç›¸å®¹æ€§"""

    # æ˜ç¢ºè²æ˜ Pydantic æ¬„ä½
    qdrant_client: QdrantClient
    collection_name: str
    embeddings: OpenAIEmbeddings
    chunks: List[DocumentChunk]
    meteor: MeteorUtilities

    def __init__(self, qdrant_client: QdrantClient, collection_name: str,
                 embeddings: OpenAIEmbeddings, chunks: List[DocumentChunk]):
        super().__init__(
            qdrant_client=qdrant_client,
            collection_name=collection_name,
            embeddings=embeddings,
            chunks=chunks,
            meteor=MeteorUtilities()
        )
    
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """æª¢ç´¢ç›¸é—œæ–‡ä»¶"""
        try:
            # ä½¿ç”¨ MeteorUtilities ç”ŸæˆæŸ¥è©¢å‘é‡
            query_embedding = self.meteor.get_embedding(query)
            
            # åœ¨ Qdrant ä¸­æœç´¢
            search_results = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=Config.DEFAULT_TOP_K,
                with_payload=True,
                with_vectors=False
            )
            
            documents = []
            for result in search_results:
                payload = result.payload
                
                # é‡å»º DocumentChunk - æ ¹æ“šæ˜¯å¦æœ‰åœ–ç‰‡ä½¿ç”¨ä¸åŒçš„é¡åˆ¥
                if payload.get("has_images", False):
                    from src.processors.pdf_processor import VisionDocumentChunk
                    chunk = VisionDocumentChunk(
                        page_num=payload["page_num"],
                        topic=payload["topic"],
                        sub_topic=payload["sub_topic"],
                        content=payload["content"],
                        content_type=payload["content_type"],
                        keywords=payload["keywords"],
                        difficulty_level=payload["difficulty_level"],
                        chunk_id=payload["chunk_id"],
                        has_images=payload["has_images"],
                        image_analysis=payload.get("image_analysis", ""),
                        technical_symbols=payload.get("technical_symbols", []),
                        image_path=payload.get("image_path", "")
                    )
                else:
                    chunk = DocumentChunk(
                        page_num=payload["page_num"],
                        topic=payload["topic"],
                        sub_topic=payload["sub_topic"],
                        content=payload["content"],
                        content_type=payload["content_type"],
                        keywords=payload["keywords"],
                        difficulty_level=payload["difficulty_level"],
                        chunk_id=payload["chunk_id"]
                    )
                
                # å‰µå»º LangChain Document
                doc = Document(
                    page_content=f"{chunk.topic} - {chunk.sub_topic}\n{chunk.content}",
                    metadata={
                        "chunk": chunk,
                        "similarity_score": float(result.score),
                        "page_num": chunk.page_num,
                        "topic": chunk.topic,
                        "sub_topic": chunk.sub_topic,
                        "content_type": chunk.content_type,
                        "difficulty_level": chunk.difficulty_level,
                        "has_images": getattr(chunk, 'has_images', False),
                        "image_path": getattr(chunk, 'image_path', ''),
                        "image_description": getattr(chunk, 'image_description', '')
                    }
                )
                documents.append(doc)
            
            return documents
            
        except Exception as e:
            logger.error(f"æª¢ç´¢å¤±æ•—: {e}")
            return []

class TeachingRAGSystemLangChain:
    """æ•™å­¸å‹RAGç³»çµ± - ä½¿ç”¨ LangChain æ¡†æ¶"""

    def __init__(self, openai_api_key: str = None, qdrant_url: str = None):
        """åˆå§‹åŒ–RAGç³»çµ±"""
        # è¨­ç½® OpenAI API
        api_key = openai_api_key or Config.OPENAI_API_KEY
        if not api_key:
            raise ValueError("è«‹è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸æˆ–åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")

        # åˆå§‹åŒ– LangChain çµ„ä»¶
        self.llm = ChatOpenAI(
            api_key=api_key,
            model=Config.OPENAI_MODEL,
            temperature=Config.TEMPERATURE,
            max_tokens=Config.MAX_TOKENS,
            verbose=Config.LANGCHAIN_VERBOSE
        )
        
        self.embeddings = OpenAIEmbeddings(
            api_key=api_key,
            model=Config.OPENAI_EMBEDDING_MODEL
        )
        
        # è¨­ç½® Qdrant å®¢æˆ¶ç«¯
        self.qdrant_url = qdrant_url or Config.QDRANT_URL
        self.qdrant_client = QdrantClient(url=self.qdrant_url)
        self.collection_name = Config.QDRANT_COLLECTION_NAME
        
        # åˆå§‹åŒ–å…¶ä»–çµ„ä»¶
        self.chunks: List[DocumentChunk] = []
        self.meteor = MeteorUtilities()
        self.retriever = None
        self.qa_chain = None
        self.memory = ConversationBufferWindowMemory(
            k=Config.MEMORY_WINDOW_SIZE,
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # æ•™å­¸æ¨¡å¼é…ç½®
        self.teaching_modes = {
            "qa": "å•ç­”æ¨¡å¼ - å›ç­”å…·é«”å•é¡Œ",
            "quiz": "æ¸¬é©—æ¨¡å¼ - ç”Ÿæˆæ¸¬é©—é¡Œç›®", 
            "guide": "å°è®€æ¨¡å¼ - ç« ç¯€é‡é»èªªæ˜",
            "search": "æœå°‹æ¨¡å¼ - é—œéµå­—æŸ¥è©¢",
            "explain": "è§£é‡‹æ¨¡å¼ - æ·±åº¦èªªæ˜æ¦‚å¿µ"
        }
        
        logger.info("LangChain RAG ç³»çµ±åˆå§‹åŒ–å®Œæˆ")

    def load_processed_chunks(self, file_path: str):
        """è¼‰å…¥è™•ç†éçš„æ–‡ä»¶æ®µè½"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    chunk_data = json.loads(line.strip())
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰åœ–ç‰‡è³‡è¨Š
                    if chunk_data.get("has_images", False):
                        chunk = DocumentChunk(
                            page_num=chunk_data["page_num"],
                            topic=chunk_data["topic"],
                            sub_topic=chunk_data["sub_topic"],
                            content=chunk_data["content"],
                            content_type=chunk_data["content_type"],
                            keywords=chunk_data["keywords"],
                            difficulty_level=chunk_data["difficulty_level"],
                            chunk_id=chunk_data["chunk_id"],
                            has_images=chunk_data["has_images"],
                            image_path=chunk_data.get("image_path", ""),
                            image_description=chunk_data.get("image_description", "")
                        )
                    else:
                        chunk = DocumentChunk(
                            page_num=chunk_data["page_num"],
                            topic=chunk_data["topic"],
                            sub_topic=chunk_data["sub_topic"],
                            content=chunk_data["content"],
                            content_type=chunk_data["content_type"],
                            keywords=chunk_data["keywords"],
                            difficulty_level=chunk_data["difficulty_level"],
                            chunk_id=chunk_data["chunk_id"]
                        )
                    
                    self.chunks.append(chunk)
            
            logger.info(f"æˆåŠŸè¼‰å…¥ {len(self.chunks)} å€‹æ–‡ä»¶æ®µè½")
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æ–‡ä»¶æ®µè½å¤±æ•—: {e}")
            raise

    def has_vector_data(self) -> bool:
        """æª¢æŸ¥ Qdrant é›†åˆæ˜¯å¦åŒ…å«å‘é‡è³‡æ–™"""
        try:
            collections = self.qdrant_client.get_collections()
            collection_exists = any(col.name == self.collection_name for col in collections.collections)
            
            if not collection_exists:
                return False
            
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            existing_count = collection_info.vectors_count
            points_count = collection_info.points_count
            
            if existing_count is None:
                existing_count = points_count if points_count is not None else 0
            
            return existing_count > 0
            
        except Exception as e:
            logger.warning(f"æª¢æŸ¥å‘é‡è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def should_load_local_chunks(self) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦è¼‰å…¥æœ¬åœ°æ–‡ä»¶æ®µè½"""
        return not self.has_vector_data()

    def setup_retriever(self):
        """è¨­ç½®æª¢ç´¢å™¨å’ŒQAéˆ"""
        # å¦‚æœæ²’æœ‰æœ¬åœ° chunksï¼Œå˜—è©¦å¾ Qdrant è¼‰å…¥ä¸€äº›æ¨£æœ¬ä¾†åˆå§‹åŒ–æª¢ç´¢å™¨
        if not self.chunks:
            logger.info("æ²’æœ‰æœ¬åœ°æ–‡ä»¶æ®µè½ï¼Œå˜—è©¦å¾ Qdrant è¼‰å…¥æ¨£æœ¬æ•¸æ“š...")
            self._load_sample_chunks_from_qdrant()

        # å‰µå»ºè‡ªå®šç¾©æª¢ç´¢å™¨
        self.retriever = QdrantRetriever(
            qdrant_client=self.qdrant_client,
            collection_name=self.collection_name,
            embeddings=self.embeddings,
            chunks=self.chunks
        )
        
        # è¨­ç½®æç¤ºè©æ¨¡æ¿
        qa_prompt = PromptTemplate(
            template="""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å“ç®¡æ•™è‚²è¨“ç·´è¬›å¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ•™æå…§å®¹å›ç­”å­¸å“¡å•é¡Œã€‚

æ•™æå…§å®¹ï¼š
{context}

å­¸å“¡å•é¡Œï¼š{question}

è«‹ç”¨æ¸…æ¥šã€è‡ªç„¶çš„æ–¹å¼å›ç­”å•é¡Œï¼Œæä¾›ç›¸é—œçš„é‡è¦æ¦‚å¿µèªªæ˜ï¼Œå¦‚æœæœ‰ç¯„ä¾‹æˆ–åœ–é¢èªªæ˜ä¹Ÿè«‹ä¸€ä½µæä¾›ã€‚

å›ç­”ï¼š""",
            input_variables=["context", "question"]
        )
        
        # å‰µå»ºQAéˆ
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type=Config.CHAIN_TYPE,
            retriever=self.retriever,
            return_source_documents=Config.RETURN_SOURCE_DOCUMENTS,
            chain_type_kwargs={"prompt": qa_prompt},
            verbose=Config.LANGCHAIN_VERBOSE
        )
        
        logger.info("æª¢ç´¢å™¨å’ŒQAéˆè¨­ç½®å®Œæˆ")

    def _load_sample_chunks_from_qdrant(self):
        """å¾ Qdrant è¼‰å…¥æ¨£æœ¬æ•¸æ“šä¾†åˆå§‹åŒ–æª¢ç´¢å™¨"""
        try:
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = self.qdrant_client.get_collections()
            collection_exists = any(col.name == self.collection_name for col in collections.collections)

            if not collection_exists:
                logger.warning(f"Qdrant é›†åˆ {self.collection_name} ä¸å­˜åœ¨")
                return

            # ç²å–ä¸€äº›æ¨£æœ¬é»ä¾†äº†è§£æ•¸æ“šçµæ§‹
            points = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                limit=10,  # è¼‰å…¥10å€‹æ¨£æœ¬å°±è¶³å¤ äº†
                with_payload=True,
                with_vectors=False
            )

            sample_chunks = []
            for point in points[0]:
                payload = point.payload

                # é‡å»º DocumentChunk - æ ¹æ“šæ˜¯å¦æœ‰åœ–ç‰‡ä½¿ç”¨ä¸åŒçš„é¡åˆ¥
                if payload.get("has_images", False):
                    from src.processors.pdf_processor import VisionDocumentChunk
                    chunk = VisionDocumentChunk(
                        page_num=payload.get("page_num", 0),
                        topic=payload.get("topic", ""),
                        sub_topic=payload.get("sub_topic", ""),
                        content=payload.get("content", ""),
                        content_type=payload.get("content_type", ""),
                        keywords=payload.get("keywords", []),
                        difficulty_level=payload.get("difficulty_level", ""),
                        chunk_id=payload.get("chunk_id", ""),
                        has_images=payload.get("has_images", False),
                        image_analysis=payload.get("image_analysis", ""),
                        technical_symbols=payload.get("technical_symbols", []),
                        image_path=payload.get("image_path", "")
                    )
                else:
                    chunk = DocumentChunk(
                        page_num=payload.get("page_num", 0),
                        topic=payload.get("topic", ""),
                        sub_topic=payload.get("sub_topic", ""),
                        content=payload.get("content", ""),
                        content_type=payload.get("content_type", ""),
                        keywords=payload.get("keywords", []),
                        difficulty_level=payload.get("difficulty_level", ""),
                        chunk_id=payload.get("chunk_id", "")
                    )

                sample_chunks.append(chunk)

            self.chunks = sample_chunks
            logger.info(f"å¾ Qdrant è¼‰å…¥äº† {len(sample_chunks)} å€‹æ¨£æœ¬æ–‡ä»¶æ®µè½")

        except Exception as e:
            logger.error(f"å¾ Qdrant è¼‰å…¥æ¨£æœ¬æ•¸æ“šå¤±æ•—: {e}")
            # å‰µå»ºä¸€å€‹ç©ºçš„ chunk ä½œç‚ºå¾Œå‚™
            self.chunks = []

    def create_embeddings(self):
        """å‰µå»ºå‘é‡åµŒå…¥ - ä¿æŒèˆ‡åŸç³»çµ±ç›¸å®¹çš„æ¥å£"""
        if not self.has_vector_data():
            logger.info("Qdrant é›†åˆç‚ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œéœ€è¦ç”Ÿæˆå‘é‡åµŒå…¥")
            self._generate_and_store_embeddings()
        else:
            logger.info("Qdrant é›†åˆå·²å­˜åœ¨ä¸”åŒ…å«è³‡æ–™ï¼Œè·³éå‘é‡ç”Ÿæˆ")

        # è¨­ç½®æª¢ç´¢å™¨
        self.setup_retriever()

    def _generate_and_store_embeddings(self):
        """ç”Ÿæˆä¸¦å„²å­˜å‘é‡åµŒå…¥"""
        if not self.chunks:
            raise ValueError("è«‹å…ˆè¼‰å…¥æ–‡ä»¶æ®µè½")

        try:
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å‰‡å‰µå»º
            collections = self.qdrant_client.get_collections()
            collection_exists = any(col.name == self.collection_name for col in collections.collections)

            if not collection_exists:
                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=Config.EMBEDDING_DIMENSION,
                        distance=models.Distance.COSINE
                    )
                )
                logger.info(f"å‰µå»ºæ–°é›†åˆ: {self.collection_name}")

            # æº–å‚™æ–‡å­—å…§å®¹é€²è¡Œå‘é‡åŒ–
            texts = []
            points = []

            for i, chunk in enumerate(self.chunks):
                # çµ„åˆå¤šå€‹æ¬„ä½ä»¥æé«˜æª¢ç´¢æ•ˆæœ
                combined_text = f"{chunk.topic} {chunk.sub_topic} {chunk.content}"
                if chunk.keywords:
                    combined_text += f" é—œéµå­—: {' '.join(chunk.keywords)}"
                texts.append(combined_text)

            # ä½¿ç”¨ MeteorUtilities ç”ŸæˆåµŒå…¥å‘é‡
            logger.info("æ­£åœ¨ä½¿ç”¨ MeteorUtilities ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings_list = []

            for i, text in enumerate(texts):
                embedding = self.meteor.get_embedding(text)
                embeddings_list.append(embedding)

                if (i + 1) % 10 == 0:
                    logger.info(f"å·²ç”Ÿæˆ {i + 1}/{len(texts)} å€‹å‘é‡")

            # æº–å‚™ Qdrant points
            for i, (chunk, embedding) in enumerate(zip(self.chunks, embeddings_list)):
                payload = {
                    "page_num": chunk.page_num,
                    "topic": chunk.topic,
                    "sub_topic": chunk.sub_topic,
                    "content": chunk.content,
                    "content_type": chunk.content_type,
                    "keywords": chunk.keywords,
                    "difficulty_level": chunk.difficulty_level,
                    "chunk_id": chunk.chunk_id
                }

                # æ·»åŠ åœ–ç‰‡è³‡è¨Šï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(chunk, 'has_images') and chunk.has_images:
                    payload.update({
                        "has_images": chunk.has_images,
                        "image_path": getattr(chunk, 'image_path', ''),
                        "image_description": getattr(chunk, 'image_description', '')
                    })
                else:
                    payload["has_images"] = False

                point = models.PointStruct(
                    id=i,
                    vector=embedding,
                    payload=payload
                )
                points.append(point)

            # æ‰¹é‡ä¸Šå‚³åˆ° Qdrant
            logger.info(f"æ­£åœ¨ä¸Šå‚³ {len(points)} å€‹å‘é‡åˆ° Qdrant...")
            self.qdrant_client.upsert(
                collection_name=self.collection_name,
                points=points
            )

            logger.info(f"å‘é‡åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œå…± {len(points)} å€‹å‘é‡")

        except Exception as e:
            logger.error(f"ç”Ÿæˆå‘é‡åµŒå…¥å¤±æ•—: {e}")
            raise

    def retrieve_relevant_chunks(self, query: str, top_k: int = 5,
                                topic_filter: str = None) -> List[RetrievalResult]:
        """æª¢ç´¢ç›¸é—œæ–‡ä»¶æ®µè½ - ä¿æŒèˆ‡åŸç³»çµ±ç›¸å®¹çš„æ¥å£"""
        try:
            if not self.retriever:
                self.setup_retriever()

            # ä½¿ç”¨ LangChain æª¢ç´¢å™¨
            documents = self.retriever.get_relevant_documents(query)

            # è½‰æ›ç‚ºåŸç³»çµ±çš„ RetrievalResult æ ¼å¼
            results = []
            for doc in documents[:top_k]:
                chunk = doc.metadata["chunk"]
                similarity_score = doc.metadata["similarity_score"]

                # ç”Ÿæˆç›¸é—œæ€§è§£é‡‹
                relevance_reason = self._explain_relevance(query, chunk, similarity_score)

                result = RetrievalResult(
                    chunk=chunk,
                    similarity_score=similarity_score,
                    relevance_reason=relevance_reason
                )
                results.append(result)

            # æ‡‰ç”¨ä¸»é¡Œéæ¿¾å™¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if topic_filter:
                results = [r for r in results if r.chunk.topic == topic_filter]

            logger.info(f"æ‰¾åˆ° {len(results)} å€‹ç›¸é—œæ–‡ä»¶æ®µè½")
            return results

        except Exception as e:
            logger.error(f"æª¢ç´¢å¤±æ•—: {e}")
            return self._fallback_keyword_search(query, top_k, topic_filter)

    def _explain_relevance(self, query: str, chunk: DocumentChunk, score: float) -> str:
        """è§£é‡‹ç›¸é—œæ€§åŸå› """
        reasons = []
        query_lower = query.lower()

        if query_lower in chunk.sub_topic.lower():
            reasons.append("æ¨™é¡ŒåŒ¹é…")
        if query_lower in chunk.content.lower():
            reasons.append("å…§å®¹åŒ¹é…")
        if any(keyword.lower() in query_lower for keyword in chunk.keywords):
            reasons.append("é—œéµå­—åŒ¹é…")
        if query_lower in chunk.topic.lower():
            reasons.append("ä¸»é¡ŒåŒ¹é…")

        if not reasons:
            reasons.append(f"èªç¾©ç›¸ä¼¼åº¦: {score:.3f}")

        return "; ".join(reasons)

    def _fallback_keyword_search(self, query: str, top_k: int, topic_filter: str = None) -> List[RetrievalResult]:
        """é—œéµå­—æœç´¢ä½œç‚ºå¾Œå‚™æ–¹æ¡ˆ"""
        logger.info("ä½¿ç”¨é—œéµå­—æœç´¢ä½œç‚ºå¾Œå‚™æ–¹æ¡ˆ")
        results = []
        query_lower = query.lower()

        for chunk in self.chunks:
            if topic_filter and chunk.topic != topic_filter:
                continue

            score = 0.0
            reasons = []

            if query_lower in chunk.sub_topic.lower():
                score += 0.8
                reasons.append("æ¨™é¡ŒåŒ¹é…")

            if query_lower in chunk.content.lower():
                score += 0.6
                reasons.append("å…§å®¹åŒ¹é…")

            if any(keyword.lower() in query_lower for keyword in chunk.keywords):
                score += 0.7
                reasons.append("é—œéµå­—åŒ¹é…")

            if query_lower in chunk.topic.lower():
                score += 0.5
                reasons.append("ä¸»é¡ŒåŒ¹é…")

            if score > 0:
                results.append(RetrievalResult(
                    chunk=chunk,
                    similarity_score=score,
                    relevance_reason="; ".join(reasons)
                ))

        results.sort(key=lambda x: x.similarity_score, reverse=True)
        return results[:top_k]

    def generate_teaching_response(self, query: str, mode: str = "qa",
                                 topic_filter: str = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ•™å­¸å‹å›ç­” - ä¿æŒèˆ‡åŸç³»çµ±ç›¸å®¹çš„æ¥å£"""
        # æª¢ç´¢ç›¸é—œå…§å®¹
        relevant_chunks = self.retrieve_relevant_chunks(query, top_k=3, topic_filter=topic_filter)

        if not relevant_chunks:
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ•™æä¸­æ‰¾ä¸åˆ°ç›¸é—œå…§å®¹ã€‚è«‹å˜—è©¦å…¶ä»–å•é¡Œæˆ–é—œéµå­—ã€‚",
                "mode": mode,
                "sources": []
            }

        # æ ¹æ“šæ¨¡å¼ç”Ÿæˆä¸åŒé¡å‹çš„å›ç­”
        if mode == "qa":
            response = self._generate_qa_response_langchain(query, relevant_chunks)
        elif mode == "quiz":
            response = self._generate_quiz_response_langchain(relevant_chunks)
        elif mode == "guide":
            response = self._generate_guide_response_langchain(relevant_chunks)
        elif mode == "explain":
            response = self._generate_explanation_response_langchain(query, relevant_chunks)
        else:
            response = self._generate_qa_response_langchain(query, relevant_chunks)

        # æº–å‚™ä¾†æºè³‡è¨Šä¸¦æ”¶é›†åœ–ç‰‡ URL
        sources = []
        image_urls = []

        for result in relevant_chunks:
            chunk = result.chunk
            source_info = {
                "page_num": chunk.page_num,
                "topic": chunk.topic,
                "sub_topic": chunk.sub_topic,
                "content_type": chunk.content_type,
                "difficulty_level": chunk.difficulty_level,
                "similarity_score": result.similarity_score,
                "relevance_reason": result.relevance_reason,
                "has_images": getattr(chunk, 'has_images', False),
                "image_path": getattr(chunk, 'image_path', ''),
                "image_analysis": getattr(chunk, 'image_analysis', '')
            }
            sources.append(source_info)

            # æ”¶é›†åœ–ç‰‡ URL
            if source_info["has_images"] and source_info["image_path"]:
                image_url = self._get_image_url(source_info["image_path"])
                if image_url and image_url not in image_urls:
                    image_urls.append(image_url)

        # åœ¨å›ç­”å¾Œé¢æ·»åŠ åœ–ç‰‡ URL
        if image_urls:
            response["answer"] += "\n\nğŸ“· ç›¸é—œåœ–ç‰‡ï¼š"
            for i, url in enumerate(image_urls, 1):
                response["answer"] += f"\n{i}. {url}"

        response["sources"] = sources
        response["mode"] = mode

        return response

    def _get_image_url(self, image_path: str) -> str:
        """ç”Ÿæˆåœ–ç‰‡çš„å®Œæ•´ URL - ä½¿ç”¨å¾Œç«¯APIè·¯å¾‘"""
        if not image_path:
            return None

        import os
        from urllib.parse import quote

        # æå–æª”æ¡ˆåç¨±ï¼Œè™•ç†å¯èƒ½çš„è·¯å¾‘åˆ†éš”ç¬¦å•é¡Œ
        filename = os.path.basename(image_path.replace('\\', '/'))

        # å¾Œç«¯ API åŸºç¤ URL
        api_base_url = "https://uat.heph-ai.net/api/v1/JH"

        # æª”æ¡ˆåç¨±éœ€è¦ URL ç·¨ç¢¼ï¼ˆå› ç‚ºåŒ…å«ä¸­æ–‡ï¼‰
        encoded_filename = quote(filename)

        # ç”Ÿæˆå¾Œç«¯ API URL
        image_url = f"{api_base_url}/images/{encoded_filename}"

        return image_url

    def _generate_qa_response_langchain(self, query: str, chunks: List[RetrievalResult]) -> Dict[str, Any]:
        """ä½¿ç”¨ LangChain ç”Ÿæˆå•ç­”æ¨¡å¼å›ç­”"""
        try:
            if not self.qa_chain:
                self.setup_retriever()

            # ä½¿ç”¨ LangChain QA éˆ
            result = self.qa_chain({"query": query})

            return {
                "answer": result["result"],
                "context_used": result.get("source_documents", [])
            }

        except Exception as e:
            logger.error(f"LangChain QA ç”Ÿæˆå¤±æ•—: {e}")
            # é™ç´šåˆ°åŸå§‹æ–¹æ³•
            return self._generate_qa_response_fallback(query, chunks)

    def _generate_qa_response_fallback(self, query: str, chunks: List[RetrievalResult]) -> Dict[str, Any]:
        """é™ç´šçš„å•ç­”å›ç­”ç”Ÿæˆ"""
        # çµ„åˆç›¸é—œå…§å®¹
        context_parts = []
        for result in chunks:
            chunk = result.chunk
            context_parts.append(f"ã€{chunk.topic} - {chunk.sub_topic}ã€‘\n{chunk.content}")

        context = "\n\n".join(context_parts)

        # æ§‹å»ºæç¤ºè©
        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å“ç®¡æ•™è‚²è¨“ç·´è¬›å¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ•™æå…§å®¹å›ç­”å­¸å“¡å•é¡Œã€‚

æ•™æå…§å®¹ï¼š
{context}

å­¸å“¡å•é¡Œï¼š{query}

è«‹ç”¨æ¸…æ¥šã€è‡ªç„¶çš„æ–¹å¼å›ç­”å•é¡Œï¼Œæä¾›ç›¸é—œçš„é‡è¦æ¦‚å¿µèªªæ˜ï¼Œå¦‚æœæœ‰ç¯„ä¾‹æˆ–åœ–é¢èªªæ˜ä¹Ÿè«‹ä¸€ä½µæä¾›ã€‚

å›ç­”ï¼š"""

        # ç›´æ¥èª¿ç”¨ LLM
        response = self.llm.invoke(prompt)

        return {
            "answer": response.content,
            "context_used": context
        }

    def _generate_quiz_response_langchain(self, chunks: List[RetrievalResult]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¸¬é©—æ¨¡å¼å…§å®¹"""
        main_chunk = chunks[0].chunk

        prompt = f"""åŸºæ–¼ä»¥ä¸‹æ•™æå…§å®¹ï¼Œè«‹ç”Ÿæˆ3é“æ¸¬é©—é¡Œç›®ï¼š

æ•™æå…§å®¹ï¼š
ã€{main_chunk.topic} - {main_chunk.sub_topic}ã€‘
{main_chunk.content}

è«‹ç”Ÿæˆï¼š
1. 1é“é¸æ“‡é¡Œï¼ˆ4å€‹é¸é …ï¼‰
2. 1é“æ˜¯éé¡Œ
3. 1é“ç°¡ç­”é¡Œ

æ¯é¡Œéƒ½è¦åŒ…å«æ­£ç¢ºç­”æ¡ˆå’Œç°¡è¦è§£é‡‹ã€‚

æ¸¬é©—é¡Œç›®ï¼š"""

        response = self.llm.invoke(prompt)

        return {
            "answer": response.content,
            "quiz_topic": main_chunk.topic,
            "quiz_subtopic": main_chunk.sub_topic
        }

    def _generate_guide_response_langchain(self, chunks: List[RetrievalResult]) -> Dict[str, Any]:
        """ç”Ÿæˆå°è®€æ¨¡å¼å…§å®¹"""
        # æŒ‰ä¸»é¡Œçµ„ç¹”å…§å®¹
        topics = {}
        for result in chunks:
            topic = result.chunk.topic
            if topic not in topics:
                topics[topic] = []
            topics[topic].append(result.chunk)

        guide_content = []
        for topic, topic_chunks in topics.items():
            guide_content.append(f"## {topic}")
            for chunk in topic_chunks:
                guide_content.append(f"### {chunk.sub_topic}")
                guide_content.append(chunk.content)
                if chunk.keywords:
                    guide_content.append(f"**é—œéµå­—ï¼š** {', '.join(chunk.keywords)}")
                guide_content.append("")

        return {
            "answer": "\n".join(guide_content),
            "topics_covered": list(topics.keys())
        }

    def _generate_explanation_response_langchain(self, query: str, chunks: List[RetrievalResult]) -> Dict[str, Any]:
        """ç”Ÿæˆæ·±åº¦è§£é‡‹æ¨¡å¼å…§å®¹"""
        context = "\n\n".join([f"ã€{r.chunk.topic}ã€‘{r.chunk.content}" for r in chunks])

        prompt = f"""è«‹å°ä»¥ä¸‹æ¦‚å¿µé€²è¡Œæ·±åº¦è§£é‡‹ï¼ŒåŒ…æ‹¬å®šç¾©ã€ç”¨é€”ã€æ³¨æ„äº‹é …ç­‰ï¼š

ç›¸é—œæ•™æå…§å®¹ï¼š
{context}

è¦è§£é‡‹çš„æ¦‚å¿µï¼š{query}

è«‹æä¾›ï¼š
1. æ¸…æ¥šçš„å®šç¾©
2. å¯¦éš›æ‡‰ç”¨å ´æ™¯
3. å¸¸è¦‹éŒ¯èª¤æˆ–æ³¨æ„äº‹é …
4. ç›¸é—œæ¦‚å¿µçš„é—œè¯æ€§

è©³ç´°è§£é‡‹ï¼š"""

        response = self.llm.invoke(prompt)

        return {
            "answer": response.content,
            "explanation_depth": "detailed"
        }

    def chat_with_memory(self, query: str, session_id: str = "default") -> Dict[str, Any]:
        """å¸¶è¨˜æ†¶çš„èŠå¤©åŠŸèƒ½"""
        try:
            # æª¢ç´¢ç›¸é—œå…§å®¹
            relevant_chunks = self.retrieve_relevant_chunks(query, top_k=3)

            if relevant_chunks:
                # æº–å‚™ä¸Šä¸‹æ–‡
                context = "\n\n".join([
                    f"ã€{result.chunk.topic} - {result.chunk.sub_topic}ã€‘\n{result.chunk.content}"
                    for result in relevant_chunks
                ])

                # å‰µå»ºå¸¶è¨˜æ†¶çš„å°è©±éˆ
                if not hasattr(self, 'conversation_chain') or self.conversation_chain is None:
                    self.conversation_chain = ConversationalRetrievalChain.from_llm(
                        llm=self.llm,
                        retriever=self.retriever,
                        memory=self.memory,
                        return_source_documents=True,
                        verbose=Config.LANGCHAIN_VERBOSE
                    )

                # ç”Ÿæˆå›ç­”
                result = self.conversation_chain({"question": query})

                return {
                    "answer": result["answer"],
                    "sources": [doc.metadata for doc in result.get("source_documents", [])],
                    "session_id": session_id
                }
            else:
                # æ²’æœ‰ç›¸é—œå…§å®¹æ™‚çš„ç´”å°è©±
                response = self.llm.invoke(f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å“ç®¡æ•™è‚²è¨“ç·´è¬›å¸«åŠ©æ‰‹ã€‚

ç”¨æˆ¶å•é¡Œï¼š{query}

è«‹ç”¨å‹å–„ã€å°ˆæ¥­çš„æ–¹å¼å›ç­”ã€‚å¦‚æœå•é¡Œèˆ‡å“ç®¡æ•™è‚²è¨“ç·´ç„¡é—œï¼Œè«‹å¼•å°ç”¨æˆ¶æå•ç›¸é—œå•é¡Œã€‚

å›ç­”ï¼š""")

                return {
                    "answer": response.content,
                    "sources": [],
                    "session_id": session_id
                }

        except Exception as e:
            logger.error(f"è¨˜æ†¶èŠå¤©å¤±æ•—: {e}")
            return {
                "answer": f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}",
                "sources": [],
                "session_id": session_id
            }

    def clear_memory(self):
        """æ¸…é™¤è¨˜æ†¶"""
        self.memory.clear()
        logger.info("è¨˜æ†¶å·²æ¸…é™¤")

    def get_memory_summary(self) -> Dict[str, Any]:
        """ç²å–è¨˜æ†¶æ‘˜è¦"""
        try:
            messages = self.memory.chat_memory.messages
            return {
                "message_count": len(messages),
                "memory_buffer": str(self.memory.buffer) if hasattr(self.memory, 'buffer') else "",
                "last_messages": [msg.content for msg in messages[-4:]] if messages else []
            }
        except Exception as e:
            logger.error(f"ç²å–è¨˜æ†¶æ‘˜è¦å¤±æ•—: {e}")
            return {"message_count": 0, "memory_buffer": "", "last_messages": []}

    def force_recreate_embeddings(self):
        """å¼·åˆ¶é‡æ–°å‰µå»ºå‘é‡åµŒå…¥"""
        if not self.chunks:
            raise ValueError("è«‹å…ˆè¼‰å…¥æ–‡ä»¶æ®µè½")

        logger.info(f"å¼·åˆ¶é‡æ–°å‰µå»ºé›†åˆ {self.collection_name} çš„å‘é‡åµŒå…¥...")

        try:
            # åˆªé™¤ç¾æœ‰é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                self.qdrant_client.delete_collection(self.collection_name)
                logger.info(f"å·²åˆªé™¤ç¾æœ‰é›†åˆ: {self.collection_name}")
            except Exception:
                logger.info(f"é›†åˆ {self.collection_name} ä¸å­˜åœ¨ï¼Œå°‡å‰µå»ºæ–°é›†åˆ")

            # å‰µå»ºæ–°é›†åˆ
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=Config.EMBEDDING_DIMENSION,
                    distance=models.Distance.COSINE
                )
            )

            # å¼·åˆ¶é‡æ–°ç”Ÿæˆå‘é‡
            self._generate_and_store_embeddings()

            # é‡æ–°è¨­ç½®æª¢ç´¢å™¨
            self.setup_retriever()

        except Exception as e:
            logger.error(f"å¼·åˆ¶é‡æ–°å‰µå»ºå‘é‡åµŒå…¥å¤±æ•—: {e}")
            raise

    def get_system_info(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±è³‡è¨Š"""
        return {
            "framework": "LangChain",
            "llm_model": Config.OPENAI_MODEL,
            "embedding_model": Config.OPENAI_EMBEDDING_MODEL,
            "vector_store": "Qdrant",
            "collection_name": self.collection_name,
            "chunks_loaded": len(self.chunks),
            "has_vector_data": self.has_vector_data(),
            "memory_type": type(self.memory).__name__,
            "memory_window_size": Config.MEMORY_WINDOW_SIZE,
            "teaching_modes": list(self.teaching_modes.keys())
        }


# ç‚ºäº†ä¿æŒå‘å¾Œç›¸å®¹æ€§ï¼Œå‰µå»ºä¸€å€‹åˆ¥å
TeachingRAGSystem = TeachingRAGSystemLangChain


if __name__ == "__main__":
    # ä½¿ç”¨ç¯„ä¾‹
    rag_system = TeachingRAGSystemLangChain()

    # è¼‰å…¥è™•ç†éçš„æ–‡ä»¶æ®µè½
    if os.path.exists("processed_chunks.jsonl"):
        rag_system.load_processed_chunks("processed_chunks.jsonl")

        # å»ºç«‹å‘é‡ç´¢å¼•
        rag_system.create_embeddings()

        # æ¸¬è©¦ä¸åŒæ¨¡å¼çš„æŸ¥è©¢
        test_queries = [
            ("ä»€éº¼æ˜¯æœ‰æ•ˆåœ–é¢ï¼Ÿ", "qa"),
            ("Î¦ç¬¦è™Ÿä»£è¡¨ä»€éº¼ï¼Ÿ", "explain"),
            ("é›¶ä»¶åœ–", "guide"),
            ("æ¸¬è©¦æˆ‘å°åœ–é¢ç¬¦è™Ÿçš„ç†è§£", "quiz")
        ]

        for query, mode in test_queries:
            print(f"\n=== æŸ¥è©¢: {query} (æ¨¡å¼: {mode}) ===")
            response = rag_system.generate_teaching_response(query, mode)
            print(f"å›ç­”: {response['answer'][:200]}...")
            print(f"ä¾†æºæ•¸é‡: {len(response['sources'])}")

        # æ¸¬è©¦è¨˜æ†¶èŠå¤©
        print(f"\n=== è¨˜æ†¶èŠå¤©æ¸¬è©¦ ===")
        chat_response = rag_system.chat_with_memory("ä½ å¥½ï¼Œæˆ‘æƒ³å­¸ç¿’åœ–é¢è­˜åˆ¥")
        print(f"èŠå¤©å›ç­”: {chat_response['answer'][:200]}...")

        # ç³»çµ±è³‡è¨Š
        print(f"\n=== ç³»çµ±è³‡è¨Š ===")
        info = rag_system.get_system_info()
        for key, value in info.items():
            print(f"{key}: {value}")
    else:
        print("æœªæ‰¾åˆ° processed_chunks.jsonl æª”æ¡ˆ")
