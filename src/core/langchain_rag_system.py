"""
LangChain Parent-Child RAGÁ≥ªÁµ±
‰ΩøÁî®LangChainÁöÑParentDocumentRetrieverÂØ¶ÁèæÊõ¥ÂÑ™ÂåñÁöÑÊ™¢Á¥¢
"""

import logging
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path

from langchain.retrievers import ParentDocumentRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.storage import InMemoryStore
from langchain_core.stores import BaseStore
from langchain_community.vectorstores import Qdrant
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

from config.config import Config

logger = logging.getLogger(__name__)

@dataclass
class LangChainRetrievalResult:
    """LangChainÊ™¢Á¥¢ÁµêÊûú - ÂÖºÂÆπÂéüÊúâÊ†ºÂºè"""
    document: Document
    similarity_score: float
    relevance_reason: str
    parent_content: str
    child_content: str

    # ÂÖºÂÆπÂéüÊúâÊ†ºÂºèÁöÑÂ±¨ÊÄß
    @property
    def child_chunk(self):
        """ÂÖºÂÆπÂéüÊúâchild_chunkÂ±¨ÊÄß"""
        return type('ChildChunk', (), {
            'content': self.child_content,
            'topic': self.document.metadata.get('topic', ''),
            'sub_topic': self.document.metadata.get('sub_topic', ''),
            'page_num': self.document.metadata.get('page_num', 0),
            'keywords': self.document.metadata.get('keywords', []),
            'has_images': self.document.metadata.get('has_images', False),
            'image_path': self.document.metadata.get('image_path', ''),
            'content_type': self.document.metadata.get('content_type', 'Êú™ÊåáÂÆö'),
            'source_filename': self.document.metadata.get('source_filename', '')
        })()

    @property
    def parent_chunk(self):
        """ÂÖºÂÆπÂéüÊúâparent_chunkÂ±¨ÊÄß"""
        return type('ParentChunk', (), {
            'content': self.parent_content,
            'topic': self.document.metadata.get('topic', ''),
            'page_range': (self.document.metadata.get('page_num', 0), self.document.metadata.get('page_num', 0)),
            'has_images': self.document.metadata.get('has_images', False),
            'image_paths': [self.document.metadata.get('image_path', '')] if self.document.metadata.get('image_path') else [],
            'content_type': self.document.metadata.get('content_type', 'Êú™ÊåáÂÆö'),
            'source_filename': self.document.metadata.get('source_filename', '')
        })()

class QdrantDocStore(BaseStore[str, str]):
    """QdrantÊñáÊ™îÂ≠òÂÑ≤Âô®ÔºåÁî®ÊñºÂ≠òÂÑ≤Áà∂ÊñáÊ™î"""
    
    def __init__(self, qdrant_client: QdrantClient, collection_name: str):
        self.qdrant_client = qdrant_client
        self.collection_name = f"{collection_name}_docstore"
        self._ensure_collection()
    
    def _ensure_collection(self):
        """Á¢∫‰øùÈõÜÂêàÂ≠òÂú®"""
        try:
            collections = self.qdrant_client.get_collections().collections
            collection_names = [col.name for col in collections]
            
            if self.collection_name not in collection_names:
                # ÂâµÂª∫ÊñáÊ™îÂ≠òÂÑ≤ÈõÜÂêàÔºà‰∏çÈúÄË¶ÅÂêëÈáèÔºåÂè™Â≠òÂÑ≤ÊñáÊ™îÔºâ
                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=1,  # ÊúÄÂ∞èÂêëÈáèÁ∂≠Â∫¶
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"‚úÖ ÂâµÂª∫ÊñáÊ™îÂ≠òÂÑ≤ÈõÜÂêà: {self.collection_name}")
        except Exception as e:
            logger.error(f"ÂâµÂª∫ÊñáÊ™îÂ≠òÂÑ≤ÈõÜÂêàÂ§±Êïó: {e}")
    
    def mget(self, keys: List[str]) -> List[Optional[str]]:
        """ÊâπÈáèÁç≤ÂèñÊñáÊ™î"""
        try:
            results = []
            for key in keys:
                points = self.qdrant_client.scroll(
                    collection_name=self.collection_name,
                    scroll_filter={
                        "must": [{"key": "doc_id", "match": {"value": key}}]
                    },
                    limit=1,
                    with_payload=True
                )[0]
                
                if points:
                    results.append(points[0].payload.get("content"))
                else:
                    results.append(None)
            return results
        except Exception as e:
            logger.error(f"ÊâπÈáèÁç≤ÂèñÊñáÊ™îÂ§±Êïó: {e}")
            return [None] * len(keys)
    
    def mset(self, key_value_pairs: List[tuple]) -> None:
        """ÊâπÈáèË®≠ÁΩÆÊñáÊ™î"""
        try:
            from qdrant_client.models import PointStruct
            
            points = []
            for i, (key, value) in enumerate(key_value_pairs):
                point = PointStruct(
                    id=hash(key) % (2**63),  # ÁîüÊàêÂîØ‰∏ÄID
                    vector=[0.0],  # Âç†‰ΩçÂêëÈáè
                    payload={
                        "doc_id": key,
                        "content": value
                    }
                )
                points.append(point)
            
            if points:
                self.qdrant_client.upsert(
                    collection_name=self.collection_name,
                    points=points
                )
                logger.info(f"‚úÖ ÊâπÈáèÂ≠òÂÑ≤ {len(points)} ÂÄãÊñáÊ™î")
        except Exception as e:
            logger.error(f"ÊâπÈáèË®≠ÁΩÆÊñáÊ™îÂ§±Êïó: {e}")
    
    def mdelete(self, keys: List[str]) -> None:
        """ÊâπÈáèÂà™Èô§ÊñáÊ™î"""
        try:
            for key in keys:
                self.qdrant_client.delete(
                    collection_name=self.collection_name,
                    points_selector={
                        "filter": {
                            "must": [{"key": "doc_id", "match": {"value": key}}]
                        }
                    }
                )
            logger.info(f"‚úÖ ÊâπÈáèÂà™Èô§ {len(keys)} ÂÄãÊñáÊ™î")
        except Exception as e:
            logger.error(f"ÊâπÈáèÂà™Èô§ÊñáÊ™îÂ§±Êïó: {e}")

    def yield_keys(self, prefix: Optional[str] = None):
        """ËøîÂõûÈçµÂÄºÁöÑËø≠‰ª£Âô®"""
        try:
            if prefix:
                scroll_filter = {
                    "must": [{"key": "doc_id", "match": {"text": f"{prefix}*"}}]
                }
            else:
                scroll_filter = None

            points, _ = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                scroll_filter=scroll_filter,
                limit=1000,
                with_payload=True
            )

            for point in points:
                yield point.payload.get("doc_id")
        except Exception as e:
            logger.error(f"Áç≤ÂèñÈçµÂÄºËø≠‰ª£Âô®Â§±Êïó: {e}")
            return iter([])

class LangChainParentChildRAG:
    """Âü∫ÊñºLangChainÁöÑParent-Child RAGÁ≥ªÁµ±"""
    
    def __init__(self, collection_name: str):
        self.collection_name = collection_name
        self.child_collection_name = f"{collection_name}_langchain_children"
        self.parent_collection_name = f"{collection_name}_langchain_parents"
        
        # ÂàùÂßãÂåñQdrantÂÆ¢Êà∂Á´Ø
        self.qdrant_client = QdrantClient(url=Config.QDRANT_URL)
        
        # ÂàùÂßãÂåñÂµåÂÖ•Ê®°Âûã
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=Config.OPENAI_API_KEY,
            model=Config.OPENAI_EMBEDDING_MODEL
        )
        
        # Á¢∫‰øùÂ≠êÈõÜÂêàÂ≠òÂú®
        self._ensure_child_collection()

        # ÂàùÂßãÂåñÂêëÈáèÂ≠òÂÑ≤
        self.vectorstore = Qdrant(
            client=self.qdrant_client,
            collection_name=self.child_collection_name,
            embeddings=self.embeddings
        )
        
        # ÂàùÂßãÂåñÊñáÊ™îÂ≠òÂÑ≤
        self.docstore = QdrantDocStore(self.qdrant_client, collection_name)
        
        # ÂàùÂßãÂåñÊñáÊú¨ÂàÜÂâ≤Âô® - Êõ¥‰øùÂÆàÁöÑÂèÉÊï∏
        self.parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,  # Èôç‰ΩéÁà∂ÊÆµËêΩÂ§ßÂ∞è
            chunk_overlap=150,
            separators=["\n\n", "\n", "„ÄÇ", "ÔºÅ", "Ôºü", "Ôºõ", " ", ""]
        )
        
        self.child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,   # Â¢ûÂä†Â≠êÊÆµËêΩÂ§ßÂ∞è
            chunk_overlap=50,
            separators=["\n\n", "\n", "„ÄÇ", "ÔºÅ", "Ôºü", "Ôºõ", " ", ""]
        )
        
        # ÂàùÂßãÂåñÊ™¢Á¥¢Âô®
        self.retriever = ParentDocumentRetriever(
            vectorstore=self.vectorstore,
            docstore=self.docstore,
            child_splitter=self.child_splitter,
            parent_splitter=self.parent_splitter,
            search_kwargs={"k": 15}  # Â¢ûÂä†Ê™¢Á¥¢Êï∏Èáè
        )
        
        logger.info(f"‚úÖ LangChain Parent-Child RAGÁ≥ªÁµ±ÂàùÂßãÂåñÂÆåÊàê")
        logger.info(f"  Â≠êÊÆµËêΩÈõÜÂêà: {self.child_collection_name}")
        logger.info(f"  Áà∂ÊÆµËêΩÂ≠òÂÑ≤: {self.parent_collection_name}")

    def _ensure_child_collection(self):
        """Á¢∫‰øùÂ≠êÈõÜÂêàÂ≠òÂú®"""
        try:
            collections = self.qdrant_client.get_collections().collections
            collection_names = [col.name for col in collections]

            if self.child_collection_name not in collection_names:
                # Áç≤ÂèñÂµåÂÖ•Ê®°ÂûãÁöÑÁ∂≠Â∫¶
                embedding_dimension = self._get_embedding_dimension()

                # ÂâµÂª∫Â≠êÈõÜÂêàÁî®ÊñºÂêëÈáèÂ≠òÂÑ≤
                self.qdrant_client.create_collection(
                    collection_name=self.child_collection_name,
                    vectors_config=VectorParams(
                        size=embedding_dimension,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"‚úÖ ÂâµÂª∫Â≠êÊÆµËêΩÈõÜÂêà: {self.child_collection_name} (Á∂≠Â∫¶: {embedding_dimension})")
            else:
                logger.info(f"‚úÖ Â≠êÊÆµËêΩÈõÜÂêàÂ∑≤Â≠òÂú®: {self.child_collection_name}")
        except Exception as e:
            logger.error(f"ÂâµÂª∫Â≠êÊÆµËêΩÈõÜÂêàÂ§±Êïó: {e}")
            raise

    def _get_embedding_dimension(self) -> int:
        """Áç≤ÂèñÂµåÂÖ•Ê®°ÂûãÁöÑÁ∂≠Â∫¶"""
        model_dimensions = {
            "text-embedding-ada-002": 1536,
            "text-embedding-3-small": 1536,
            "text-embedding-3-large": 3072
        }

        return model_dimensions.get(Config.OPENAI_EMBEDDING_MODEL, 1536)
    
    def add_documents_from_zerox(self, zerox_chunks: List) -> Dict[str, Any]:
        """ÂæûZeroxËôïÁêÜÁµêÊûúÊ∑ªÂä†ÊñáÊ™î"""
        try:
            # ËΩâÊèõZerox chunksÁÇ∫LangChain Documents
            documents = []
            for chunk in zerox_chunks:
                # ÁµÑÂêàÂÖßÂÆπÔºöÂåÖÂê´ÂúñÁâáÂàÜÊûêÂíåmetadata
                content_parts = [chunk.content]
                
                if hasattr(chunk, 'image_analysis') and chunk.image_analysis:
                    content_parts.append(f"\n\n**ÂúñÁâáÂàÜÊûê**: {chunk.image_analysis}")
                
                if hasattr(chunk, 'technical_symbols') and chunk.technical_symbols:
                    content_parts.append(f"\n\n**ÊäÄË°ìÁ¨¶Ëôü**: {', '.join(chunk.technical_symbols)}")
                
                full_content = "".join(content_parts)
                
                # ÂâµÂª∫LangChain Document
                doc = Document(
                    page_content=full_content,
                    metadata={
                        "page_num": chunk.page_num,
                        "topic": chunk.topic,
                        "sub_topic": chunk.sub_topic,
                        "content_type": chunk.content_type,
                        "keywords": chunk.keywords,
                        "difficulty_level": chunk.difficulty_level,
                        "chunk_id": chunk.chunk_id,
                        "has_images": getattr(chunk, 'has_images', False),
                        "image_path": getattr(chunk, 'image_path', ''),
                        "source_filename": getattr(chunk, 'source_filename', ''),
                        "source": f"page_{chunk.page_num}"
                    }
                )
                documents.append(doc)
            
            # Ê∑ªÂä†ÊñáÊ™îÂà∞Ê™¢Á¥¢Âô®
            logger.info(f"üîÑ ÈñãÂßãËôïÁêÜ {len(documents)} ÂÄãÊñáÊ™î...")
            start_time = time.time()
            
            self.retriever.add_documents(documents)
            
            processing_time = time.time() - start_time
            
            # Áµ±Ë®à‰ø°ÊÅØ
            result = {
                "success": True,
                "original_chunks": len(zerox_chunks),
                "documents_added": len(documents),
                "processing_time": processing_time,
                "child_collection": self.child_collection_name,
                "parent_collection": self.parent_collection_name
            }
            
            logger.info(f"‚úÖ LangChainËôïÁêÜÂÆåÊàê:")
            logger.info(f"  ÂéüÂßãÊÆµËêΩ: {result['original_chunks']}")
            logger.info(f"  Ê∑ªÂä†ÊñáÊ™î: {result['documents_added']}")
            logger.info(f"  ËôïÁêÜÊôÇÈñì: {result['processing_time']:.2f}Áßí")
            
            return result
            
        except Exception as e:
            logger.error(f"Ê∑ªÂä†ÊñáÊ™îÂ§±Êïó: {e}")
            return {"success": False, "error": str(e)}
    
    def retrieve_relevant_chunks(self, query: str, top_k: int = 10) -> List[LangChainRetrievalResult]:
        """Ê™¢Á¥¢Áõ∏ÈóúÊÆµËêΩ - ÁúüÊ≠£ÁöÑÁà∂Â≠êÈóú‰øÇÊ™¢Á¥¢"""
        try:
            logger.info(f"üîç LangChainÊ™¢Á¥¢Êü•Ë©¢: {query}")

            # Ê≠•È©ü1: ÂÖàÂú®Â≠êÊÆµËêΩ‰∏≠ÊêúÁ¥¢ÔºåÁç≤ÂèñÁõ∏ÈóúÁöÑÂ≠êÊÆµËêΩ
            child_docs = self.vectorstore.similarity_search_with_score(query, k=top_k*2)
            logger.info(f"üîç Âú®Â≠êÊÆµËêΩ‰∏≠ÊâæÂà∞ {len(child_docs)} ÂÄãÁõ∏ÈóúÁµêÊûú")

            # Ê≠•È©ü2: Ê†πÊìöÂ≠êÊÆµËêΩÁöÑIDÁç≤ÂèñÂ∞çÊáâÁöÑÁà∂ÊÆµËêΩ
            results = []
            processed_parent_ids = set()  # ÈÅøÂÖçÈáçË§áÁöÑÁà∂ÊÆµËêΩ

            for child_doc, score in child_docs:
                try:
                    # Áç≤ÂèñÂ≠êÊÆµËêΩÁöÑÁà∂ÊñáÊ™îID
                    parent_id = child_doc.metadata.get('doc_id', '')
                    logger.debug(f"Â≠êÊÆµËêΩmetadata: {child_doc.metadata}")
                    logger.debug(f"Áà∂ÊñáÊ™îID: {parent_id}")

                    if not parent_id or parent_id in processed_parent_ids:
                        # Â¶ÇÊûúÊ≤íÊúâÁà∂IDÔºåÁõ¥Êé•‰ΩøÁî®Â≠êÊÆµËêΩÂÖßÂÆπ‰ΩúÁÇ∫Áà∂ÂÖßÂÆπ
                        if not parent_id:
                            logger.debug("Ê≤íÊúâÊâæÂà∞Áà∂ÊñáÊ™îIDÔºå‰ΩøÁî®Â≠êÊÆµËêΩÂÖßÂÆπ")
                            parent_content = child_doc.page_content
                        else:
                            continue
                    else:
                        processed_parent_ids.add(parent_id)

                        # ÂæûdocstoreÁç≤ÂèñÂÆåÊï¥ÁöÑÁà∂ÊÆµËêΩ
                        parent_docs = self.docstore.mget([parent_id])
                        logger.debug(f"ÂæûdocstoreÁç≤ÂèñÁöÑÁà∂ÊñáÊ™î: {parent_docs}")

                        if parent_docs and parent_docs[0]:
                            parent_content = parent_docs[0]
                            logger.debug(f"Áà∂ÊÆµËêΩÈï∑Â∫¶: {len(parent_content)}")
                        else:
                            logger.debug("docstoreËøîÂõûÁ©∫Ôºå‰ΩøÁî®Â≠êÊÆµËêΩÂÖßÂÆπ‰ΩúÁÇ∫Áà∂ÂÖßÂÆπ")
                            parent_content = child_doc.page_content

                    # Ë®àÁÆóÁõ∏‰ººÂ∫¶ÂàÜÊï∏ÔºàËΩâÊèõÁÇ∫0-1ÁØÑÂúçÔºâ
                    similarity_score = max(0.1, min(1.0, 1.0 - score))

                    # ÁîüÊàêÁõ∏ÈóúÊÄßËß£Èáã
                    relevance_reason = self._explain_relevance(query, child_doc, similarity_score)

                    # ÂâµÂª∫ÁµêÊûúÂ∞çË±°ÔºåÂåÖÂê´ÁúüÊ≠£ÁöÑÁà∂Â≠êÈóú‰øÇ
                    result = LangChainRetrievalResult(
                        document=child_doc,  # ‰øùÁïôÂéüÂßãÂ≠êÊñáÊ™îÁöÑmetadata
                        similarity_score=similarity_score,
                        relevance_reason=relevance_reason,
                        parent_content=parent_content,  # ÂÆåÊï¥ÁöÑÁà∂ÊÆµËêΩÂÖßÂÆπ
                        child_content=child_doc.page_content  # ÂåπÈÖçÁöÑÂ≠êÊÆµËêΩÂÖßÂÆπ
                    )
                    results.append(result)

                    # ÈôêÂà∂ÁµêÊûúÊï∏Èáè
                    if len(results) >= top_k:
                        break

                except Exception as doc_error:
                    logger.error(f"ËôïÁêÜÂ≠êÊñáÊ™îÊôÇÂá∫ÈåØ: {doc_error}")
                    continue

            # Â¶ÇÊûúÂ≠êÊÆµËêΩÊ™¢Á¥¢Â§±ÊïóÔºåÂõûÈÄÄÂà∞ÂéüÂßãÊñπÊ≥ï
            if not results:
                logger.warning("Â≠êÊÆµËêΩÊ™¢Á¥¢Â§±ÊïóÔºåÂõûÈÄÄÂà∞ParentDocumentRetriever")
                docs = self.retriever.get_relevant_documents(query)
                docs = docs[:top_k]

                for i, doc in enumerate(docs):
                    similarity_score = max(0.1, 1.0 - (i * 0.1))
                    relevance_reason = self._explain_relevance(query, doc, similarity_score)

                    result = LangChainRetrievalResult(
                        document=doc,
                        similarity_score=similarity_score,
                        relevance_reason=relevance_reason,
                        parent_content=doc.page_content,
                        child_content=doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
                    )
                    results.append(result)

            logger.info(f"‚úÖ LangChainÁà∂Â≠êÊ™¢Á¥¢ÂÆåÊàêÔºåÊâæÂà∞ {len(results)} ÂÄãÁõ∏ÈóúÁµêÊûú")
            logger.info(f"   Áà∂ÊÆµËêΩÂπ≥ÂùáÈï∑Â∫¶: {sum(len(r.parent_content) for r in results) // len(results) if results else 0} Â≠óÁ¨¶")
            logger.info(f"   Â≠êÊÆµËêΩÂπ≥ÂùáÈï∑Â∫¶: {sum(len(r.child_content) for r in results) // len(results) if results else 0} Â≠óÁ¨¶")

            return results

        except Exception as e:
            logger.error(f"LangChainÊ™¢Á¥¢Â§±Êïó: {e}")
            import traceback
            logger.error(f"Ë©≥Á¥∞ÈåØË™§: {traceback.format_exc()}")
            return []
    
    def _explain_relevance(self, query: str, doc: Document, score: float) -> str:
        """ÁîüÊàêÁõ∏ÈóúÊÄßËß£Èáã"""
        reasons = []
        
        if score > 0.7:
            reasons.append("È´òÂ∫¶Áõ∏Èóú")
        elif score > 0.4:
            reasons.append("‰∏≠Â∫¶Áõ∏Èóú")
        else:
            reasons.append("‰ΩéÂ∫¶Áõ∏Èóú")
        
        # Ê™¢Êü•ÈóúÈçµÂ≠óÂåπÈÖç
        query_lower = query.lower()
        content_lower = doc.page_content.lower()
        
        if any(word in content_lower for word in query_lower.split()):
            reasons.append("ÈóúÈçµÂ≠óÂåπÈÖç")
        
        # Ê™¢Êü•metadataÂåπÈÖç
        metadata = doc.metadata
        if metadata.get('topic') and any(word in metadata['topic'].lower() for word in query_lower.split()):
            reasons.append("‰∏ªÈ°åÂåπÈÖç")
        
        return "; ".join(reasons)
    
    def has_vector_data(self) -> bool:
        """Ê™¢Êü•ÊòØÂê¶ÊúâÂêëÈáèÊï∏Êìö"""
        try:
            collections = self.qdrant_client.get_collections().collections
            collection_names = [col.name for col in collections]
            
            if self.child_collection_name in collection_names:
                child_info = self.qdrant_client.get_collection(self.child_collection_name)
                child_count = child_info.vectors_count or child_info.points_count or 0
                return child_count > 0
            
            return False
        except Exception as e:
            logger.warning(f"Ê™¢Êü•ÂêëÈáèÊï∏ÊìöÊôÇÁôºÁîüÈåØË™§: {e}")
            return False
    
    def generate_answer(self, query: str, top_k: int = 10) -> Dict[str, Any]:
        """ÁîüÊàêÂõûÁ≠î - ÂÖºÂÆπÂéüÊúâ API"""
        try:
            # Ê™¢Á¥¢Áõ∏ÈóúÊÆµËêΩ
            retrieval_results = self.retrieve_relevant_chunks(query, top_k)

            if not retrieval_results:
                return {
                    "answer": "Êä±Ê≠âÔºåÊàëÁÑ°Ê≥ïÂú®Áü•Ë≠òÂ∫´‰∏≠ÊâæÂà∞Áõ∏ÈóúË≥áË®ä‰æÜÂõûÁ≠îÊÇ®ÁöÑÂïèÈ°å„ÄÇ",
                    "sources": [],
                    "query": query
                }

            # Ê∫ñÂÇô‰∏ä‰∏ãÊñá
            context_parts = []
            sources = []

            for result in retrieval_results:
                # ‰ΩøÁî®Áà∂ÊÆµËêΩ‰ΩúÁÇ∫‰∏ä‰∏ãÊñá
                context_parts.append(result.parent_content)

                # ÊßãÂª∫‰æÜÊ∫êË≥áË®ä
                source_info = {
                    "has_images": result.document.metadata.get('has_images', False),
                    "image_paths": [result.document.metadata.get('image_path', '')] if result.document.metadata.get('image_path') else [],
                    "page_num": result.document.metadata.get('page_num', 0),
                    "topic": result.document.metadata.get('topic', ''),
                    "content": result.child_content,
                    "similarity_score": result.similarity_score
                }
                sources.append(source_info)

            # ‰ΩøÁî® OpenAI ÁîüÊàêÂõûÁ≠î
            from openai import OpenAI
            client = OpenAI(api_key=Config.OPENAI_API_KEY)

            context = "\n\n".join(context_parts[:5])  # ÈôêÂà∂‰∏ä‰∏ãÊñáÈï∑Â∫¶

            prompt = f"""Âü∫Êñº‰ª•‰∏ãÊïôÊùêÂÖßÂÆπÂõûÁ≠îÂïèÈ°åÔºö

{context}

ÂïèÈ°åÔºö{query}

Ë´ãÊ†πÊìöÊïôÊùêÂÖßÂÆπÊèê‰æõÊ∫ñÁ¢∫„ÄÅË©≥Á¥∞ÁöÑÂõûÁ≠î„ÄÇÂ¶ÇÊûúÊïôÊùê‰∏≠ÊúâÁõ∏ÈóúÂúñÁâáÊàñÂúñË°®ÔºåË´ãÂú®ÂõûÁ≠î‰∏≠ÊèêÂèä„ÄÇ"""

            completion = client.chat.completions.create(
                model=Config.OPENAI_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=Config.MAX_TOKENS,
                temperature=Config.TEMPERATURE
            )

            answer = completion.choices[0].message.content

            return {
                "answer": answer,
                "sources": sources,
                "query": query,
                "retrieval_count": len(retrieval_results)
            }

        except Exception as e:
            logger.error(f"ÁîüÊàêÂõûÁ≠îÂ§±Êïó: {e}")
            return {
                "answer": f"Êä±Ê≠âÔºåËôïÁêÜÊÇ®ÁöÑÂïèÈ°åÊôÇÁôºÁîüÈåØË™§Ôºö{str(e)}",
                "sources": [],
                "query": query
            }

    def get_system_info(self) -> Dict[str, Any]:
        """Áç≤ÂèñÁ≥ªÁµ±Ë≥áË®ä"""
        return {
            "system_type": "LangChain Parent-Child RAG",
            "collection_name": self.collection_name,
            "child_collection": self.child_collection_name,
            "parent_collection": self.parent_collection_name,
            "embedding_model": Config.OPENAI_EMBEDDING_MODEL,
            "llm_model": Config.OPENAI_MODEL,
            "chunking_strategy": "langchain_parent_child",
            "parent_chunk_size": 1500,
            "child_chunk_size": 400
        }
